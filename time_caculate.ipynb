{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"/Users/tonykuo/Downloads/harness_cache_20240428\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk through the cache path\n",
    "all_cache_paths = []\n",
    "for root, dirs, files in os.walk(cache_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pt\"):\n",
    "            all_cache_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cache_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    \"mmlu_flan_n_shot_generative\",\n",
    "    # \"truthfulqa_gen\",\n",
    "    # \"gsm8k\",\n",
    "    # \"bbh_fewshot\",\n",
    "    # \"triviaqa\",\n",
    "    # \"nq_open\"\n",
    "]\n",
    "\n",
    "ranker_name = [\n",
    "    \"llm_blender\",\n",
    "    \"oassterm\",\n",
    "    \"ultrarm\"\n",
    "]\n",
    "\n",
    "analize_data = [\n",
    "    {\n",
    "        \"types\": \"single_model\", \n",
    "        \"display_name\": \"openchat/openchat_3.5\",\n",
    "        \"model_name\":[\"openchat/openchat_3.5\"]\n",
    "    },\n",
    "    {\n",
    "        \"types\": \"single_model\", \n",
    "        \"display_name\": \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\",\n",
    "        \"model_name\":[\"NousResearch/Nous-Hermes-2-SOLAR-10.7B\"]\n",
    "    },\n",
    "    {\n",
    "        \"types\": \"single_model\", \n",
    "        \"display_name\": \"FuseAI/OpenChat-3.5-7B-Solar\",\n",
    "        \"model_name\":[\"FuseAI/OpenChat-3.5-7B-Solar\"]\n",
    "    },\n",
    "    # {\n",
    "    #     \"types\": \"ranker\", \n",
    "    #     \"display_name\": \"llm_blender\",\n",
    "    #     \"model_name\":[\"openchat/openchat_3.5\", \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\"],\n",
    "    #     \"ranker_name\": \"llm_blender\"\n",
    "    # },\n",
    "    {\n",
    "        \"types\": \"ranker\", \n",
    "        \"display_name\": \"oassterm\",\n",
    "        \"model_name\": [\"openchat/openchat_3.5\", \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\"],\n",
    "        \"ranker_name\": \"oassterm\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"types\": \"ranker\", \n",
    "    #     \"display_name\": \"ultrarm\",\n",
    "    #     \"model_name\":[\"openchat/openchat_3.5\", \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\"],\n",
    "    #     \"ranker_name\": \"ultrarm\"\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_split_v4(s):\n",
    "    skip_str = [\n",
    "        \"mmlu_flan_n_shot_generative\",\n",
    "        \"truthfulqa_gen\",\n",
    "        \"gsm8k\",\n",
    "        \"bbh_fewshot\",\n",
    "        \"triviaqa\",\n",
    "        \"nq_open\",\n",
    "        \"llm_blender\",\n",
    "        \"openchat-openchat_3.5\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    parts = s.split('_')\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(parts):\n",
    "        current = parts[i]\n",
    "        match_found = False\n",
    "\n",
    "        # Check if the current part can form a task with the next few parts\n",
    "        for j in range(1, len(parts) - i):\n",
    "            combined = '_'.join(parts[i:i+j+1])\n",
    "            if combined in skip_str:\n",
    "                result.append(combined)\n",
    "                i += j  # Move the index to the end of the combined part\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            result.append(current)\n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_cache_paths(all_cache_paths) -> pd.DataFrame:\n",
    "    accept_tasks = [\n",
    "        \"mmlu_flan_n_shot_generative\",\n",
    "        \"truthfulqa_gen\",\n",
    "        \"gsm8k\",\n",
    "        \"bbh_fewshot\",\n",
    "        \"triviaqa\",\n",
    "        \"nq_open\"\n",
    "    ]\n",
    "    pd_parse_data = [custom_split_v4(os.path.basename(path)[:-3]) for path in all_cache_paths]\n",
    "\n",
    "    pd_ranker_names = []\n",
    "    for d in pd_parse_data:\n",
    "        if d[0] in ranker_name:\n",
    "            pd_ranker_names.append(d[0])\n",
    "            d.pop(0)\n",
    "        else:\n",
    "            pd_ranker_names.append(None)\n",
    "\n",
    "    pd_task_names = []\n",
    "    for d in pd_parse_data:\n",
    "        if d[0] in accept_tasks:\n",
    "            # print(d)\n",
    "            pd_task_names.append(d[0])\n",
    "            d.pop(0)\n",
    "        else:\n",
    "            raise ValueError(\"Task name not found\")\n",
    "        \n",
    "\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"cache_path\": all_cache_paths,\n",
    "            \"ranker_names\": pd_ranker_names,\n",
    "            \"task_names\": pd_task_names,\n",
    "            \"models\": pd_parse_data\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cache_paths_df = parse_cache_paths(all_cache_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cache_path</th>\n",
       "      <th>ranker_names</th>\n",
       "      <th>task_names</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>oassterm</td>\n",
       "      <td>triviaqa</td>\n",
       "      <td>[meta-llama-Meta-Llama-3-8B-Instruct, mistrala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>ultrarm</td>\n",
       "      <td>truthfulqa_gen</td>\n",
       "      <td>[NousResearch-Nous-Hermes-2-SOLAR-10.7B, openc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>None</td>\n",
       "      <td>mmlu_flan_n_shot_generative</td>\n",
       "      <td>[FuseAI-OpenChat-3.5-7B-Solar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>oassterm</td>\n",
       "      <td>mmlu_flan_n_shot_generative</td>\n",
       "      <td>[meta-llama-Meta-Llama-3-8B-Instruct, mistrala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>ultrarm</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>[NousResearch-Nous-Hermes-2-SOLAR-10.7B, openc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>None</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>[openchat-openchat_3.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>None</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>[openchat-openchat_3.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>llm_blender</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>[openchat-openchat_3.5, NousResearch-Nous-Herm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>None</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>[NousResearch-Nous-Hermes-2-Mixtral-8x7B-DPO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>/Users/tonykuo/Downloads/harness_cache_2024042...</td>\n",
       "      <td>None</td>\n",
       "      <td>nq_open</td>\n",
       "      <td>[FuseAI-OpenChat-3.5-7B-Mixtral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cache_path ranker_names  \\\n",
       "0   /Users/tonykuo/Downloads/harness_cache_2024042...     oassterm   \n",
       "1   /Users/tonykuo/Downloads/harness_cache_2024042...      ultrarm   \n",
       "2   /Users/tonykuo/Downloads/harness_cache_2024042...         None   \n",
       "3   /Users/tonykuo/Downloads/harness_cache_2024042...     oassterm   \n",
       "4   /Users/tonykuo/Downloads/harness_cache_2024042...      ultrarm   \n",
       "..                                                ...          ...   \n",
       "91  /Users/tonykuo/Downloads/harness_cache_2024042...         None   \n",
       "92  /Users/tonykuo/Downloads/harness_cache_2024042...         None   \n",
       "93  /Users/tonykuo/Downloads/harness_cache_2024042...  llm_blender   \n",
       "94  /Users/tonykuo/Downloads/harness_cache_2024042...         None   \n",
       "95  /Users/tonykuo/Downloads/harness_cache_2024042...         None   \n",
       "\n",
       "                     task_names  \\\n",
       "0                      triviaqa   \n",
       "1                truthfulqa_gen   \n",
       "2   mmlu_flan_n_shot_generative   \n",
       "3   mmlu_flan_n_shot_generative   \n",
       "4                       nq_open   \n",
       "..                          ...   \n",
       "91                      nq_open   \n",
       "92                        gsm8k   \n",
       "93                      nq_open   \n",
       "94                        gsm8k   \n",
       "95                      nq_open   \n",
       "\n",
       "                                               models  \n",
       "0   [meta-llama-Meta-Llama-3-8B-Instruct, mistrala...  \n",
       "1   [NousResearch-Nous-Hermes-2-SOLAR-10.7B, openc...  \n",
       "2                      [FuseAI-OpenChat-3.5-7B-Solar]  \n",
       "3   [meta-llama-Meta-Llama-3-8B-Instruct, mistrala...  \n",
       "4   [NousResearch-Nous-Hermes-2-SOLAR-10.7B, openc...  \n",
       "..                                                ...  \n",
       "91                            [openchat-openchat_3.5]  \n",
       "92                            [openchat-openchat_3.5]  \n",
       "93  [openchat-openchat_3.5, NousResearch-Nous-Herm...  \n",
       "94      [NousResearch-Nous-Hermes-2-Mixtral-8x7B-DPO]  \n",
       "95                   [FuseAI-OpenChat-3.5-7B-Mixtral]  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cache_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filter(cache_df: pd.DataFrame, task:str, models:list[str], ranker_name:str|None= None) -> str:\n",
    "    cache_df = cache_df.copy()\n",
    "    if ranker_name is not None:\n",
    "        assert  len(models) > 1, \"ranker_name should have more than 1 model_name\"\n",
    "    \n",
    "    \n",
    "    if ranker_name is None:\n",
    "        cache_df = cache_df[cache_df[\"ranker_names\"].isna()]\n",
    "    else:\n",
    "        cache_df = cache_df[cache_df[\"ranker_names\"] == ranker_name]\n",
    "\n",
    "    cache_df = cache_df[cache_df[\"task_names\"] == task]\n",
    "\n",
    "    # all models in cache_df[\"models\"], regardless of the order\n",
    "    cache_df[\"models\"] = cache_df[\"models\"].apply(sorted)\n",
    "    cache_df[\"models\"] = cache_df[\"models\"].apply(lambda x: tuple(x))\n",
    "    cache_df = cache_df[cache_df[\"models\"] == tuple(sorted(models))]\n",
    "\n",
    "    # print(cache_df.shape)\n",
    "    assert len(cache_df) == 1, \"There should be only one cache path\"\n",
    "    \n",
    "    return cache_df.cache_path.to_list()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(cache_path: str, pop_first: bool = True) -> dict:\n",
    "    assert os.path.exists(cache_path), f\"Cache path not found, {cache_path}\"\n",
    "    cache = torch.load(cache_path)\n",
    "    if pop_first:\n",
    "        _ = cache.pop(list(cache.keys())[0])\n",
    "    return cache\n",
    "\n",
    "\n",
    "def calculate_from_cache_path(cache_path: str, ranker: bool, pop_first: bool = True) -> Tuple[float, int]:    \n",
    "    cache = load_cache(cache_path, pop_first=pop_first)\n",
    "    \n",
    "    total_latency = 0\n",
    "    total_generated_token = 0\n",
    "    for _, data in cache.items():\n",
    "        if ranker:\n",
    "            # print(data)\n",
    "            total_latency += data['latency']\n",
    "        else:\n",
    "            total_latency += data['latency']['generate_latency']\n",
    "            total_generated_token += data['latency']['num_generate_tokens']\n",
    "        \n",
    "    return total_latency, total_generated_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_flow(data:dict, task:str):\n",
    "    target_models = [target_model.replace(\"/\", \"-\") for target_model in data[\"model_name\"]]\n",
    "\n",
    "    cache_path = parse_filter(\n",
    "        cache_df=all_cache_paths_df,\n",
    "        task=task,\n",
    "        models=target_models,\n",
    "        ranker_name=data.get(\"ranker_name\", None)\n",
    "    )\n",
    "    total_latency, total_generated_token = calculate_from_cache_path(\n",
    "        cache_path=cache_path,\n",
    "        ranker=data[\"types\"] == \"ranker\",\n",
    "        pop_first=True\n",
    "    )\n",
    "    return total_latency, total_generated_token\n",
    "\n",
    "def ranker_flow(data:dict, task:str):\n",
    "    target_models = [target_model.replace(\"/\", \"-\") for target_model in data[\"model_name\"]]\n",
    "\n",
    "    # init dict\n",
    "    cache_path = parse_filter(\n",
    "        cache_df=all_cache_paths_df,\n",
    "        task=task,\n",
    "        models=target_models,\n",
    "        ranker_name=data.get(\"ranker_name\", None)\n",
    "    )\n",
    "\n",
    "    ranker_result = load_cache(cache_path, pop_first=True)  \n",
    "    \"\"\"\n",
    "    {\"...question...\":\n",
    "        {\n",
    "            \"candidate_dict\": {\n",
    "                \"openchat/openchat_3.5\": {\n",
    "                    \"text\": \"(A)\",\n",
    "                    \"latency\": 0.05516314506530762\n",
    "                },\n",
    "                \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\": {\n",
    "                    \"text\": \"(C)\",\n",
    "                    \"latency\": 0.05516314506530762\n",
    "                }\n",
    "            },\n",
    "            \"final_candidate_model\": \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\",\n",
    "            \"final_rank_result\": \"(C)\"\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    for target_model in data[\"model_name\"]:\n",
    "        tm = target_model.replace(\"/\", \"-\")\n",
    "        cache_path = parse_filter(\n",
    "            cache_df=all_cache_paths_df,\n",
    "            task=task,\n",
    "            models=[tm],\n",
    "            ranker_name=None\n",
    "        )\n",
    "        # print(cache_path)\n",
    "        target_model_result = load_cache(cache_path, pop_first=True)\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"...quesion...\": {\n",
    "                \"response\": \"Bobby Scott and Bob Russell wrote the lyrics to \\\"He Ain't Heavy\",\n",
    "                \"latency\": {\n",
    "                    \"generate_latency\": 0.4029510021209717,\n",
    "                    \"num_generate_tokens\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        for q in target_model_result:\n",
    "            ranker_result[q]['candidate_dict'][target_model]['latency'] += target_model_result[q]['latency']['generate_latency']\n",
    "            ranker_result[q]['candidate_dict'][target_model]['num_generate_tokens'] = target_model_result[q]['latency']['num_generate_tokens']\n",
    "\n",
    "    # final result\n",
    "    \"\"\"\n",
    "    {\"...question...\":\n",
    "        {\n",
    "            \"candidate_dict\": {\n",
    "                \"openchat/openchat_3.5\": {\n",
    "                    \"text\": \"(A)\",\n",
    "                    \"latency\": 0.05516314506530762,\n",
    "                    \"num_generate_tokens\": 4\n",
    "                },\n",
    "                \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\": {\n",
    "                    \"text\": \"(C)\",\n",
    "                    \"latency\": 0.05516314506530762,\n",
    "                    \"num_generate_tokens\": 4\n",
    "                }\n",
    "            },\n",
    "            \"final_candidate_model\": \"NousResearch/Nous-Hermes-2-SOLAR-10.7B\",\n",
    "            \"final_rank_result\": \"(C)\"\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    total_latency = 0\n",
    "    total_generated_token = 0\n",
    "    for q in ranker_result:\n",
    "        each_latency = []\n",
    "\n",
    "        # get max latency\n",
    "        for _, v in ranker_result[q]['candidate_dict'].items():\n",
    "            each_latency.append(v['latency'])\n",
    "        total_latency += max(each_latency)\n",
    "\n",
    "        # get total generated token by final selected model\n",
    "        final_selected_model = ranker_result[q]['final_candidate_model']\n",
    "        total_generated_token += ranker_result[q]['candidate_dict'][final_selected_model]['num_generate_tokens']\n",
    "\n",
    "    return total_latency, total_generated_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result_table(analize_data:dict):\n",
    "    display_name = [data['display_name'] for data in analize_data]\n",
    "    display_name.insert(0, \"task\")\n",
    "    _df = pd.DataFrame(columns=display_name)\n",
    "\n",
    "    for task in tasks:\n",
    "        _result = {\"task\": task}\n",
    "        for data in analize_data:\n",
    "            if data[\"types\"] == \"single_model\":\n",
    "                total_latency, total_generated_token = single_model_flow(data, task)\n",
    "            elif data[\"types\"] == \"ranker\":\n",
    "                #print(f\"{data=}\")\n",
    "                total_latency, total_generated_token = ranker_flow(data, task)\n",
    "            else:\n",
    "                raise ValueError(\"types should be single_model or ranker\")\n",
    "            \n",
    "            _result[data[\"display_name\"]] = f\"{round(total_latency/total_generated_token, 3)} s/token\"\n",
    "\n",
    "        # print(_result)\n",
    "        \n",
    "        # add a row to the dataframe\n",
    "        _df = _df._append(_result, ignore_index=True)\n",
    "\n",
    "\n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>openchat/openchat_3.5</th>\n",
       "      <th>NousResearch/Nous-Hermes-2-SOLAR-10.7B</th>\n",
       "      <th>FuseAI/OpenChat-3.5-7B-Solar</th>\n",
       "      <th>oassterm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mmlu_flan_n_shot_generative</td>\n",
       "      <td>31.951 tok/s</td>\n",
       "      <td>17.876 tok/s</td>\n",
       "      <td>30.381 tok/s</td>\n",
       "      <td>18.634 tok/s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          task openchat/openchat_3.5  \\\n",
       "0  mmlu_flan_n_shot_generative          31.951 tok/s   \n",
       "\n",
       "  NousResearch/Nous-Hermes-2-SOLAR-10.7B FuseAI/OpenChat-3.5-7B-Solar  \\\n",
       "0                           17.876 tok/s                 30.381 tok/s   \n",
       "\n",
       "       oassterm  \n",
       "0  18.634 tok/s  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tb = calculate_result_table(analize_data)\n",
    "result_tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
